mutate(text = stringr::str_replace_all(.$text, "’", " "))
tr <- corpus(tweets)
docvars(tr,"Id") <- tweets$twitterId
docvars(tr,"date") <- tweets$date
docvars(tr,"auteur") <- tweets$authorId
docvars(tr,"group") <- tweets$group
corpus <- tweets %>%
mutate(text = stringr::str_replace_all(.$text, "’", " ")) %>%
.$text %>%
VectorSource()%>%
VCorpus()
corpus <- corpus %>%
tm_map(content_transformer(tolower))%>%
tm_map(stripWhitespace) %>%
tm_map(removeNumbers)%>%
tm_map(removePunctuation)%>%
tm_map(removeWords, stopwords("fr"))
TDM <- corpus %>%
TermDocumentMatrix() %>%
as.matrix()
TDM <- sort(rowSums(TDM),decreasing=TRUE)
TDM <- data.frame(word = names(TDM),freq=TDM)
barplot(height=head(TDM,10)$freq, names.arg=head(TDM,10)$word, xlab="Mots", ylab="Fréquence", col="#973232",las=2)
trtok <- tokens_tolower(trtok)
trtok <- tokens_remove(trtok,padding = FALSE) %>%
filter(!word %in% stopwords_iso$fr) %>%
count(word, sort = TRUE) %>%
head(10)
reserve <- read_json("data/reserve/2015_reserve_parlementaire.json")
require(jsonlite)
knitr::opts_chunk$set(echo = TRUE)
require("dslabs")
data("trump_tweets")
trump <- trump_tweets
require(quanteda)
require("quanteda.textmodels")
require("quanteda.textstats")
require("arrow")
library(quanteda.textmodels)
library("quanteda.textstats")
library("quanteda.textplots")
library(tidyverse)
require(tm)
library(rainette)
require("udpipe")
require(jsonlite)
tr <- corpus(
trump$text
)
docnames(tr) <- trump$id_str
docvars(tr,"source") <- trump$source
docvars(tr,"created") <- trump$created_at
docvars(tr,"retweet") <- trump$retweet_count
docvars(tr,"favorite") <- trump$favorite_count
trtok <- tokens(tr,remove_punct = T, remove_url = T,remove_separators = T)
trtok <- tokens_tolower(trtok)
trtok <- tokens_remove(trtok,stopwords("english"), padding = TRUE)
trtok <- tokens_wordstem(trtok)
fc <- fcm(trtok,context = "window", window = 5, tri = FALSE)
tt <- dfm(trtok)
top=names(topfeatures(tt, 30))
tweets <- read_parquet(file = "french_deputies_tweets.parquet") %>%
mutate(text = stringr::str_replace_all(.$text, "’", " "))
tr <- corpus(tweets)
docvars(tr,"Id") <- tweets$twitterId
docvars(tr,"date") <- tweets$date
docvars(tr,"auteur") <- tweets$authorId
docvars(tr,"group") <- tweets$group
corpus <- tweets %>%
mutate(text = stringr::str_replace_all(.$text, "’", " ")) %>%
.$text %>%
VectorSource()%>%
VCorpus()
corpus <- corpus %>%
tm_map(content_transformer(tolower))%>%
tm_map(stripWhitespace) %>%
tm_map(removeNumbers)%>%
tm_map(removePunctuation)%>%
tm_map(removeWords, stopwords("fr"))
TDM <- corpus %>%
TermDocumentMatrix() %>%
as.matrix()
TDM <- sort(rowSums(TDM),decreasing=TRUE)
TDM <- data.frame(word = names(TDM),freq=TDM)
barplot(height=head(TDM,10)$freq, names.arg=head(TDM,10)$word, xlab="Mots", ylab="Fréquence", col="#973232",las=2)
trtok <- tokens_tolower(trtok)
trtok <- tokens_remove(trtok,padding = FALSE) %>%
filter(!word %in% stopwords_iso$fr) %>%
count(word, sort = TRUE) %>%
head(10)
reserve <- read_json("data/reserve/2015_reserve_parlementaire.json")
read_json("data/reserve/2015_reserve_parlementaire.json")
reserve <- read_json("data/reserve/2014_reserve_parlementaire.json")
reserve <- read_json_arrow("data/reserve/2015_reserve_parlementaire.json")
reserve <- read_json("data/reserve/2014_reserve_parlementaire.json")
groupe <- reserve %>% group_by(Groupe)
reserve
reserve <- fromJSON("data/reserve/2014_reserve_parlementaire.json")
reserve
groupe <- reserve %>% group_by(Groupe)
res <- groupe %>% summarize(min=min(Montant),max=max(Montant),
Moyenne=mean(Montant),EC=sd(Montant),
Somme=sum(Montant),nbActeur=length(unique(ID_Acteur)),
Somme_par_acteur=Somme/nbActeur)
ggplot(res,aes(x=Groupe,y=Somme_par_acteur,fill=Groupe))+
geom_bar(,stat="identity",position="dodge")
ggplot(res %>% filter(nbActeur != 1),aes(x=Groupe,y=Somme_par_acteur,fill=Groupe))+
geom_bar(,stat="identity",position="dodge")
reserve <- reserve %>% mutate(doc_id=paste0("A",1:nrow(reserve))) %>%
filter(Groupe != "" & !is.na(Groupe))
reserve$Descriptif
reserve$Groupe
reserve <- reserve %>% mutate(doc_id=paste0("A",1:nrow(reserve))) %>%
filter(Groupe == "" | is.na(Groupe))
reserve$Groupe
reserve$Descriptif
reserve
reserve <- fromJSON("data/reserve/2014_reserve_parlementaire.json")
reserve <- reserve %>% mutate(doc_id=paste0("A",1:nrow(reserve))) %>%
filter(Groupe == "" | is.na(Groupe))
reserve$Groupe
reserve$Descriptif
reserve$sum(reserve$Montant)
reserve$Montant
sum(reserve$Montant)
sum(reserve$Montant)/nrow(reserve)
reserve <- fromJSON("data/reserve/2014_reserve_parlementaire.json")
groupe <- reserve %>% group_by(Groupe)
res <- groupe %>% summarize(min=min(Montant),max=max(Montant),
Moyenne=mean(Montant),EC=sd(Montant),
Somme=sum(Montant),nbActeur=length(unique(ID_Acteur)),
Somme_par_acteur=Somme/nbActeur)
ggplot(res,aes(x=Groupe,y=Somme_par_acteur,fill=Groupe))+
geom_bar(,stat="identity",position="dodge")
groupe2 <- groupe %>%
filter(Groupe == "" | is.na(Groupe))
groupe2 <- groupe %>%
filter(Groupe == "" | is.na(Groupe))
reserve$Groupe
sum(reserve$Montant)/nrow(reserve)
reserve_corpus <- corpus(
reserve,
text_field = "Descriptif",
docid_field = "doc_id"
)
tok <- tokens(reserve_corpus, remove_punct = TRUE, remove_numbers = TRUE)
tok <- tokens_tolower(tok)
mystopwords <- c("loire-atlantique")
tok <- tokens_remove(tok, stopwords("fr")) %>%
tokens_remove(pattern = mystopwords,valuetype = 'fixed')
dtm <- dfm(tok)
res <- rainette(dtm, k = 5)
rainette_explor(res, dtm, reserve_corpus)
reserve
groupe2 <- groupe %>%
filter(Groupe == "" | is.na(Groupe))
groupe2
reserve <- fromJSON("data/reserve/2014_reserve_parlementaire.json")
groupe <- reserve %>% group_by(Groupe)
res <- groupe %>% summarize(min=min(Montant),max=max(Montant),
Moyenne=mean(Montant),EC=sd(Montant),
Somme=sum(Montant),nbActeur=length(unique(ID_Acteur)),
Somme_par_acteur=Somme/nbActeur)
tok <- tokens(reserve_corpus, remove_punct = TRUE, remove_numbers = TRUE)
tok <- tokens_tolower(tok)
mystopwords <- c("loire-atlantique")
tok <- tokens_remove(tok, stopwords("fr")) %>%
tokens_remove(pattern = mystopwords,valuetype = 'fixed')
dtm <- dfm(tok)
textmodel_ca(dtm, smooth = 0, nd = NA, sparse = FALSE, residual_floor = 0.1)
textmodel_ca(dtm, smooth = 0, nd = NA, sparse = TRUE, residual_floor = 0.1)
ca = textmodel_ca(dtm, smooth = 0, nd = NA, sparse = TRUE, residual_floor = 0.1)
summary(ca)
rownames(ca)
ca$rownamesca
ca$rownames
ca$rowcoord
ca$rowsup
ca$colnames
library(wordcloud2)
install.packages("wordcloud2")
topall=names(topfeatures(tt, 100))
wordcloud2(topall, size = 1,shape = 'circle')
library(wordcloud2)
wordcloud2(topall, size = 1,shape = 'circle')
topall
topfeatures(tt, 30)
topall=topfeatures(tt, 300)
wordcloud2(topall, size = 1,shape = 'circle')
topallw <- data.frame(word=names(topall),freq=topall)
wordcloud2(topall, size = 1,shape = 'circle')
wordcloud2(topallw, size = 1,shape = 'circle')
wordcloud2(topallw, size = 100,shape = 'circle')
wordcloud2(topallw, size = 0.1,shape = 'circle')
wordcloud2(topallw, size = 1,shape = 'circle')
top=names(topfeatures(dtm, 30))
top=names(topfeatures(dtm, 150))
topallw <- data.frame(word=names(topall),freq=topall)
wordcloud2(topallw, size = 1,shape = 'circle')
topallw <- data.frame(word=names(top),freq=top)
top=names(topfeatures(dtm, 150))
top
top=topfeatures(dtm, 150)
topallw <- data.frame(word=names(top),freq=top)
wordcloud2(topallw, size = 1,shape = 'circle')
wordcloud2(topallw, size = 10,shape = 'circle')
wordcloud2(topallw, size = 5,shape = 'circle')
wordcloud2(topallw, size = 3,shape = 'circle')
install.packages("Textometrie")
library("Textometrie")
install.packages("textometrie")
install.packages("textometrie")
install.packages("lexicometrie")
install.packages("textometry")
library("textometry")
knitr::opts_chunk$set(echo = TRUE)
require("dslabs")
data("trump_tweets")
trump <- trump_tweets
require(quanteda)
require("quanteda.textmodels")
require("quanteda.textstats")
require("arrow")
library(quanteda.textmodels)
library("quanteda.textstats")
library("quanteda.textplots")
library(tidyverse)
require(tm)
library(rainette)
require("udpipe")
require(jsonlite)
library(wordcloud2)
library("textometry")
reserve <- fromJSON("data/reserve/2014_reserve_parlementaire.json")
knitr::opts_chunk$set(echo = TRUE)
require(quanteda)
require("quanteda.textmodels")
require("quanteda.textstats")
require("arrow")
library(quanteda.textmodels)
library("quanteda.textstats")
library("quanteda.textplots")
library(tidyverse)
require(tm)
library(rainette)
require("udpipe")
require(jsonlite)
library(wordcloud2)
library("textometry")
library(gtsummary)
reserve_corpus <- corpus(
reserve,
text_field = "Descriptif",
docid_field = "doc_id"
)
tok <- tokens(reserve_corpus, remove_punct = TRUE, remove_numbers = TRUE)
reserve_corpus <- corpus(
reserve,
text_field = "Descriptif",
docid_field = "doc_id"
)
tok <- tokens(reserve_corpus, remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower(tok)
tbl_summary(reserve%>% filter(-ID_Acteur))
tbl_summary(reserve %>% filter(-ID_Acteur))
tbl_summary(reserve %>% select(-ID_Acteur))
tbl_summary(reserve %>% select(-ID_Acteur,-Bénéficiaire))
tbl_summary(reserve %>% select(-ID_Acteur,-Bénéficiaire,-Descriptif))
table(reserve$Bénéficiaire)
tb <- table(reserve$Bénéficiaire)
names(tb)[tb>2]
data.frame(Nom=names(tb),Nombre=tb)
data.frame(Nom=names(tb1),Nombre=tb1)
tb1 <- names(tb)[tb>1]
data.frame(Nom=names(tb1),Nombre=tb1)
tb <- table(reserve$Bénéficiaire)
tb1 <- [tb>1]
tb <- table(reserve$Bénéficiaire)
tb1 <- tb>1
names(tb1)
data.frame(Nom=names(tb1),Nombre=tb1)
tb <- table(reserve$Bénéficiaire)
tb1 <- tb[tb>1]
names(tb1)
data.frame(Nom=names(tb1),Nombre=tb1)
table(reserve$Groupe)
dt <- data.frame(Nom=names(tb1),Nombre=tb1)
dt
ggplot(dt,aes(Nombre))+geom_histogram()
ggplot(dt,aes(Freq.Nombre))+geom_histogram()
ggplot(dt,aes(Nombre))+geom_histogram()
colnames(dt)
dt <- data.frame(Nom=names(tb1),Nombre=as.numeric(tb1))
colnames(dt)
dt <- data.frame(Nom=names(tb1),Nombre=as.numeric(tb1))
dt
ggplot(dt,aes(Nombre))+geom_histogram()
tbt <- table(reserve$Bénéficiaire)
tbt <- table(reserve$Bénéficiaire)
dt <- data.frame(Nom=names(tbt),Nombre=as.numeric(tbt))
ggplot(dt,aes(Nombre))+geom_histogram()
ggplot(dt,aes(Nombre))+geom_histogram()+scale_y_log10()
ggplot(reserve,aes(Groupe,Montant))+geom_bar()
ggplot(reserve,aes(Groupe,Montant))+geom_bar(stat="identity",position="dodge")
maisque <- reserve%>%groupe
maisque <- reserve %>% filter(Groupe=="")
maisque <- reserve %>% filter(Groupe=="")
maisque
table(maisque$Nom)
table(maisque$Descriptif)
table(maisque$Nom)
reserve <- reserve %>% mutate(ifelse(Groupe=="","Présidence AN",Groupe))
ggplot(reserve,aes(Groupe,Montant))+geom_bar(stat="identity",position="dodge")
reserve <- reserve %>% mutate(ifelse(Groupe=="","Présidence AN",Groupe))
ggplot(reserve,aes(Groupe,Montant))+geom_bar(stat="identity",position="dodge")
reserve <- reserve %>% mutate(Groupe=ifelse(Groupe=="","Présidence AN",Groupe))
ggplot(reserve,aes(Groupe,Montant))+geom_bar(stat="identity",position="dodge")
groupe <- reserve %>% group_by(Groupe)
res <- groupe %>% summarize(min=min(Montant),max=max(Montant),
Moyenne=mean(Montant),EC=sd(Montant),
Somme=sum(Montant),nbActeur=length(unique(ID_Acteur)),
Somme_par_acteur=Somme/nbActeur)
groupe <- reserve %>% group_by(Groupe)
res <- groupe %>% summarize(min=min(Montant),max=max(Montant),
Moyenne=mean(Montant),EC=sd(Montant),
Somme=sum(Montant),nbActeur=length(unique(ID_Acteur)),
Somme_par_acteur=Somme/nbActeur)
ggplot(res,aes(x=Groupe,y=Somme_par_acteur,fill=Groupe))+
geom_bar(,stat="identity",position="dodge")
groupe <- reserve %>% group_by(Groupe)
res <- groupe %>% summarize(min=min(Montant),max=max(Montant),
Moyenne=mean(Montant),EC=sd(Montant),
Somme=sum(Montant),nbActeur=length(unique(ID_Acteur)),
Somme_par_acteur=Somme/nbActeur)
res
require(xlsx)
require(writexl)
require(writexl)
write_xlsx(res,file="Résultats/Résultats-Deputes-Reserve.xlsx")
write_xlsx(res,"Résultats/Résultats-Deputes-Reserve.xlsx")
ggplot(res %>% filter(nbActeur != 1),aes(x=Groupe,y=Somme_par_acteur,fill=Groupe))+
geom_bar(,stat="identity",position="dodge")
ggplot(res,aes(x=Groupe,y=Somme_par_acteur,fill=Groupe))+
geom_bar(,stat="identity",position="dodge")
deputes <- fromJSON("data/reserve/AMO10_deputes_actifs_mandats_actifs_organes_XIV.json")
deputes$export
deputes <- read_json("data/reserve/AMO10_deputes_actifs_mandats_actifs_organes_XIV.json")
deputes <- deputes$export
deputes$organes
deputes$acteurs
reserve <- reserve %>% mutate(doc_id=paste0("A",1:nrow(reserve)))
reserve_corpus <- corpus(
reserve,
text_field = "Descriptif",
docid_field = "doc_id"
)
tok <- tokens(reserve_corpus, remove_punct = TRUE, remove_numbers = TRUE)
tok <- tokens_tolower(tok)
mystopwords <- c("loire-atlantique")
tok <- tokens_remove(tok, stopwords("fr")) %>%
tokens_remove(pattern = mystopwords,valuetype = 'fixed')
dtm <- dfm(tok)
res <- rainette(dtm, k = 5)
res <- rainette(dtm, k = 5,min_segment_size = 10)
rainette_explor(res, dtm, reserve_corpus)
trtok <- tokens_wordstem(tok)
tt <- dfm(trtok)
reserve <- reserve %>% mutate(doc_id=paste0("A",1:nrow(reserve)))
reserve_corpus <- corpus(
reserve,
text_field = "Descriptif",
docid_field = "doc_id"
)
tok <- tokens(reserve_corpus, remove_punct = TRUE, remove_numbers = TRUE)
tok <- tokens_tolower(tok) %>% tokens_wordstem(tok)
tok <- tokens(reserve_corpus, remove_punct = TRUE, remove_numbers = TRUE)
tok <- tokens_tolower(tok) %>% tokens_wordstem(tok,language="fr")
?tokens_wordstem
tok <- tokens_tolower(tok) %>% tokens_wordstem(tok,language="fr")
tok <- tokens_tolower() %>% tokens_wordstem(language="fr")
tok <- tokens_tolower() %>% tokens_wordstem(tok,language="fr")
tok <- tokens_tolower() %>% tokens_wordstem(language="fr")
tok <- tokens(reserve_corpus, remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower()
tok <-  tokens_wordstem(language="fr")
tok <-tokens_wordstem(tok,language="fr")
reserve_corpus <- corpus(
reserve,
text_field = "Descriptif",
docid_field = "doc_id"
)
mystopwords <- c("loire-atlantique")
tok <- tokens(reserve_corpus, remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(tok, stopwords("fr")) %>%
tokens_wordstem(tok,language="fr") %>%
tokens_remove(pattern = mystopwords,valuetype = 'fixed')
tok <- tokens(reserve_corpus, remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("fr")) %>%
tokens_wordstem(language="fr") %>%
tokens_remove(pattern = mystopwords,valuetype = 'fixed')
dtm <- dfm(tok)
res <- rainette(dtm, k = 5)
rainette_explor(res, dtm, reserve_corpus)
mystopwords <- c("loire-atlantique")
tok <- tokens(reserve_corpus, remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_remove(pattern = mystopwords,valuetype = 'fixed')
tokens_tolower() %>%
tokens_remove(stopwords("fr")) %>%
tokens_wordstem(language="fr") %>%
dtm <- dfm(tok)
tok <- tokens(reserve_corpus, remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_remove(pattern = mystopwords,valuetype = 'fixed') %>%
tokens_tolower() %>%
tokens_remove(stopwords("fr")) %>%
tokens_wordstem(language="fr")
dtm <- dfm(tok)
res <- rainette(dtm, k = 5)
rainette_explor(res, dtm, reserve_corpus)
top
top=names(topfeatures(dtm, 30))
topall=topfeatures(dtm, 300)
top
dfreq <- data.frame(word=names(topall),freq=as.numeric(topall))
wordcloud2(dfreq)
rainette_explor(res, dtm, reserve_corpus)
top
top=topfeatures(dtm, 30)
topall=topfeatures(dtm, 300)
top
wordcloud2(dfreq)
tok <- tokens(reserve_corpus, remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_remove(pattern = mystopwords,valuetype = 'fixed') %>%
tokens_tolower() %>%
tokens_remove(stopwords("fr"))
dfreq <- data.frame(word=names(topall),freq=as.numeric(topall))
wordcloud2(dfreq)
tok <- tokens(reserve_corpus, remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_remove(pattern = mystopwords,valuetype = 'fixed') %>%
tokens_tolower() %>%
tokens_remove(stopwords("fr"))
top=topfeatures(dtm, 30)
topall=topfeatures(dtm, 300)
top
tok <- tokens(reserve_corpus, remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_remove(pattern = mystopwords,valuetype = 'fixed') %>%
tokens_tolower() %>%
tokens_remove(stopwords("fr"))
dtm <- dfm(tok)
top=topfeatures(dtm, 30)
topall=topfeatures(dtm, 300)
top
dfreq <- data.frame(word=names(topall),freq=as.numeric(topall))
wordcloud2(dfreq)
wordcloud2(dfreq,scale=2)
wordcloud2(dfreq,size=2)
dfreq <- data.frame(word=names(top),freq=as.numeric(top))
wordcloud2(dfreq,size=2)
dfreq <- data.frame(word=names(topall),freq=as.numeric(topall))
wordcloud2(dfreq,size=2)
library(udpipe)
udmodel <- udpipe_download_model(language = "french")
library(udpipe)
udmodel <- udpipe_download_model(language = "french")
tok <- tokens(reserve_corpus, remove_punct = TRUE, remove_numbers = TRUE) %>%
tokens_remove(pattern = mystopwords,valuetype = 'fixed') %>%
tokens_tolower() %>%
tokens_remove(stopwords("fr"))
toktok <- lapply(tok,function(x) udpipe(x,object = udmodel)
)
toktok <- lapply(tok,function(x) ifelse(x!="",udpipe(x,object = udmodel)),"")
toktok <- lapply(tok,function(x) ifelse(x!="",udpipe(x,object = udmodel),""))
toktok
toktok2 <- lapply(toktok,function(x) x[[4]])
toktok2 <- lapply(toktok,function(x) if(length(x)>0,x[[4]],NA))
toktok2 <- lapply(toktok,function(x) ifelse(length(x)>0,x[[4]],NA))
toktok2 <- lapply(toktok,function(x) ifelse(length(x)>0,x[[4]],NA))
toktok2 <- lapply(toktok,function(x) ifelse(length(x)>1,x[[4]],NA))
toktok
toktok <- lapply(tok,function(x) list(ifelse(x!="",udpipe(x,object = udmodel),"")))
toktok
toktok2 <- lapply(toktok,function(x) ifelse(length(x)>1,x[[1]][[4]],NA))
toktok2
toktok2 <- lapply(toktok,function(x) ifelse(length(x[[1]])>0,x[[1]][[4]],NA))
dt[dt$Nombre>5,]
EEC = read.csv2("data/fd_eec19_csv/FD_csv_EEC19.csv", header = TRUE, sep = ";", quote = "\"",
dec = ".", fill = TRUE, comment.char = "")
EEC = read.csv2("data/fd_eec19_csv/FD_EEC_2019.csv", header = TRUE, sep = ";", quote = "\"",
dec = ".", fill = TRUE, comment.char = "")
summarize(EEC,proportion=sum(ACTEU %in% 1)/sum(ACTEU %in% c(2))
summarize(EEC,proportion=sum(ACTEU %in% 1)/sum(ACTEU %in% c(2)))
summarize(EEC,tauxCDD50 = 100*sum(EXTRIAN*(AGE3 %in% c(50) & STAT2 %in% c(2) & STATUTR %in% c(4)))/
sum(EXTRIAN*(AGE3 %in% c(50) & STAT2 %in% c(2))))
summarize(EEC,proportion=sum(EXTRIAN*(ACTEU %in% 1))/sum(EXTRIAN*(ACTEU %in% c(2))))
summarize(EEC,proportion=sum(EXTRIAN*(ACTEU %in% 2))/sum(EXTRIAN*(ACTEU %in% c(1,2))))
summarize(EEC,proportion=sum(EXTRIAN*(ACTEU %in% 2))/sum(EXTRIAN*(ACTEU %in% c(1,2))))*100
summarize(EEC,proportion=sum((ACTEU %in% 2))/sum((ACTEU %in% c(1,2))))*100
summarize(EEC,proportion=sum((Sexe %in% 2))/sum((Sexe %in% c(1,2))))*100
summarize(EEC,proportion=sum((SEXE %in% 2))/sum((SEXE %in% c(1,2))))*100
summarize(EEC,proportion=sum(EXTRIAN*(SEXE %in% 2))/sum(EXTRIAN*(SEXE %in% c(1,2))))*100
summarize(EEC,proportion=sum((SEXE %in% 2 & ACTEU %in% 2))/sum((SEXE %in% c(1,2))))*100
summarize(EEC,proportion=sum((SEXE %in% 2 & ACTEU %in% 2))/sum((SEXE %in% c(1,2) & ACTEU %in% c(2))))*100
summarize(EEC,proportion=sum(EXTRIAN*(SEXE %in% 2 & ACTEU %in% 2))/sum(EXTRIAN*(SEXE %in% c(1,2) & ACTEU %in% c(2))))*100
summarize(EEC,proportion=sum((SEXE %in% 2 & ACTEU %in% 2))/sum((ACTEU %in% c(1,2))))*100
summarize(EEC,proportion=sum((SEXE %in% 1 & ACTEU %in% 2))/sum((ACTEU %in% c(1,2))))*100
summarize(EEC,proportion=sum(EXTRIAN*(SEXE %in% 2 & ACTEU %in% 2))/sum(EXTRIAN*(ACTEU %in% c(1,2))))*100
summarize(EEC,proportion=sum(EXTRIAN*(SEXE %in% 1 & ACTEU %in% 2))/sum(EXTRIAN*(ACTEU %in% c(1,2))))*100
summarize(EEC,proportion=sum((SEXE %in% 2 & ACTEU %in% 2))/sum((ACTEU %in% c(1,2))))*100
summarize(EEC,proportion=sum((SEXE %in% 1 & ACTEU %in% 2))/sum((ACTEU %in% c(1,2))))*100
summarize(EEC,proportion=sum((SEXE %in% 2 & ACTEU %in% 2))/sum((ACTEU %in% c(1,2) & SEXE %in% 2)))*100
summarize(EEC,proportion=sum((SEXE %in% 1 & ACTEU %in% 2))/sum((ACTEU %in% c(1,2) & SEXE %in% 1)))*100
